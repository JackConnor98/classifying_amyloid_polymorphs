# Import Modules
import pymol
from pymol import cmd
import os
import pandas as pd
import numpy as np
from scipy.optimize import linear_sum_assignment
import string

#############################################################################################################################

# list of pdbs to import taken from pdb_names generated by "get_pdb_names.R"
with open(os.path.join("Output", "pdb_names.txt"), "r") as file:
    # Skip the first line which contains the column name
    next(file)
    # Initialize an empty list to store the names
    pdb_names = []
    # Iterate over the lines in the file
    for line in file:
        # Strip any leading/trailing whitespaces and append the name to the list
        pdb_names.append(line.strip())

# Making output directories
output_dirs = [
    os.path.join("Output", "PDBs"),
    os.path.join("Output", "PDBs", "published_structure"),
    os.path.join("Output", "PDBs", "asymetric_unit"),
    os.path.join("Output", "PDBs", "unique_chains")
]
# Checking to see if the directories exist
for dir_path in output_dirs:
    if not os.path.exists(dir_path):
        os.mkdir(dir_path)

# Initializing dataframes
com_and_fibril_df = pd.DataFrame()
com_distances_df = pd.DataFrame()
min_distance_df = pd.DataFrame()

##########################
### Defining functions ###
##########################

# Function to calculate center of mass
def calculate_center_of_mass(coords):
    x_com = np.mean(coords['x'])
    y_com = np.mean(coords['y'])
    z_com = np.mean(coords['z'])
    return x_com, y_com, z_com

# Function to calculate Euclidean distance between two points in 3D space
def calculate_distance(point1, point2):
    return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2 + (point1[2] - point2[2])**2)

# Function to calculate radius of gyration
def calculate_radius_of_gyration(coords, com):
    x_com, y_com, z_com = com
    squared_distances = [(x - x_com)**2 + (y - y_com)**2 + (z - z_com)**2 for x, y, z in zip(coords['x'], coords['y'], coords['z'])]
    rg = np.sqrt(np.mean(squared_distances))
    return rg

# Function to check if two points can be part of the same fibril
def can_be_in_fibril(i, j, distance_threshold):
    return np.sqrt((current_pdb_df.loc[i, 'x_com'] - current_pdb_df.loc[j, 'x_com'])**2 +
                (current_pdb_df.loc[i, 'y_com'] - current_pdb_df.loc[j, 'y_com'])**2 +
                (current_pdb_df.loc[i, 'z_com'] - current_pdb_df.loc[j, 'z_com'])**2) < distance_threshold

# Function to save selected chains as a .pdb file
def save_selected_chains(chains, filename):        
    
    cmd.reinitialize()
    cmd.load(os.path.join("Output", "PDBs", "published_structure", pdb + ".pdb"))

    # Removing non-protein atoms and any UNK residues
    cmd.remove("not polymer.protein")
    cmd.remove("resn UNK")

    # Remove all chains except those in the 'chains' list
    remove_statement = "chain "
    for chain in current_pdb_df['chain']:
        if chain not in chains:
            remove_statement += f"{chain}+"
    
    # Remove the trailing '+'
    remove_statement = remove_statement.rstrip('+')

    # If the remove statement is empty, no chains need to be removed
    if remove_statement != "chain ":
        cmd.remove(remove_statement)
    
    # Iterate over each chain to set the B-factor (temperature factor)
    for chain in chains:
        # Fetch the current fibril number for this chain
        fibril_number = current_pdb_df.loc[current_pdb_df['chain'] == chain, 'fibril'].iloc[0]
        
        # Set B-factor to fibril number for each atom in the chain
        cmd.alter(f"{pdb} and chain {chain}", f"b={int(fibril_number)}")

    # Orienting the structure
    cmd.orient()

    # Saving .pdb file
    cmd.save(filename)

# Function to select the chain with the lowest z_com in a fibril
def select_chain_with_lowest_z_com(fibril):
    fibril_chains = current_pdb_df[current_pdb_df['fibril'] == fibril]
    return fibril_chains.loc[fibril_chains['z_com'].idxmin(), 'chain']


################################
### Looping Through Each PDB ###
################################

# Set PyMOL to run without GUI
pymol.pymol_argv = ['pymol', '-qc']
pymol.finish_launching()

# Import First chain of PDBs
for pdb in pdb_names:

    # Reinitialize PyMol
    cmd.reinitialize()

    ##########################
    ### IMPORTING PDB DATA ###
    ##########################

    print(f"\nAnalyzing {pdb}")

    # Downloading the PDB file
    cmd.fetch(pdb)

    ###################################################################################
    # Handling poorly formatted chain names within PDBs as it can cause saving conflicts
    # This will rename chains to single-character IDs (A-Z, a-z, 0-9) = 62 total

    # Step 1: Get all chains present in the structure
    all_chains = sorted(cmd.get_chains(pdb))

    # Step 2: Prepare a list of valid final chain IDs (A-Z, a-z, 0-9) = 62 total
    available_ids = list(string.ascii_uppercase + string.ascii_lowercase + string.digits)

    if len(all_chains) > len(available_ids):
        raise ValueError("Too many chains to assign unique single-character chain IDs!")

    # Step 3: Generate temporary chain IDs (e.g., '000', '001', ...)
    temp_ids = [f"{i:03d}" for i in range(len(all_chains))]

    # Step 4: Map original chains to temp IDs, and then to final available single-char IDs
    original_to_temp = dict(zip(all_chains, temp_ids))
    temp_to_final = dict(zip(temp_ids, available_ids))

    # Step 5: First renaming pass — original → temp (e.g., 'AA' → '001')
    for original, temp in original_to_temp.items():
        cmd.alter(f"chain {original}", f"chain='{temp}'")

    # Step 6: Second renaming pass — temp → final (e.g., '001' → 'A')
    for temp, final in temp_to_final.items():
        cmd.alter(f"chain {temp}", f"chain='{final}'")
    ###################################################################################

    # Save the structure as a .pdb file in the published_structure directory
    output_path = os.path.join("Output", "PDBs", "published_structure", f"{pdb}.pdb")
    cmd.save(output_path, pdb)

    # Deleting the .cif files that automatically save
    for filename in os.listdir():
        if filename.endswith(".cif"):
            file_path = os.path.join(filename)
            os.remove(file_path)

    # Removing non-protein atoms and any UNK residues
    cmd.remove("not polymer.protein")
    cmd.remove("resn UNK")


    ###############################################
    ### Creating a dataframe from the .pdb file ###
    ###############################################

    # Initialize lists to store atom information
    chains = []
    residues = []
    atom_numbers = []
    atom_ids = []
    x_coords = []
    y_coords = []
    z_coords = []

    # Extract atom information
    atom_data = cmd.get_model(pdb)
    for atom in atom_data.atom:
        chains.append(atom.chain)
        residues.append(atom.resi)
        atom_numbers.append(atom.index)
        atom_ids.append(atom.name)
        x_coords.append(atom.coord[0])
        y_coords.append(atom.coord[1])
        z_coords.append(atom.coord[2])

    # Create a DataFrame
    data = {
        "chain": chains,
        "residue": residues,
        "atom_number": atom_numbers,
        "atom_id": atom_ids,
        "x": x_coords,
        "y": y_coords,
        "z": z_coords,
    }
    df = pd.DataFrame(data)

    ################################################
    ### Fusing Monomers Seperated into 2+ chains ###
    ################################################

    # Find C-alpha coordinates of the first and last residues of each chain
    chain_ca_coords = {}
    for chain in df['chain'].unique():
        chain_df = df[(df['chain'] == chain) & (df['atom_id'] == 'CA')]
        first_residue = chain_df.iloc[0]
        last_residue = chain_df.iloc[-1]
        chain_ca_coords[chain] = {
            'first_residue': {
                'residue': int(first_residue['residue']),
                'coords': (first_residue['x'], first_residue['y'], first_residue['z'])
            },
            'last_residue': {
                'residue': int(last_residue['residue']),
                'coords': (last_residue['x'], last_residue['y'], last_residue['z'])
            }
        }

    # Build distance matrix for the Hungarian algorithm
    chains = list(chain_ca_coords.keys())
    num_chains = len(chains)
    distance_matrix = np.full((num_chains, num_chains), np.inf)
    closest_residue_pair = []

    for i, chain in enumerate(chains):
        chain_residues = df[df['chain'] == chain]['residue'].unique().astype(int)
        for j, other_chain in enumerate(chains):
            if chain != other_chain:
                other_chain_residues = df[df['chain'] == other_chain]['residue'].unique().astype(int)
                if np.intersect1d(chain_residues, other_chain_residues).size == 0:
                    # Determine the closest residues numerically
                    chain_first_residue_num = chain_ca_coords[chain]['first_residue']['residue']
                    chain_last_residue_num = chain_ca_coords[chain]['last_residue']['residue']
                    other_chain_first_residue_num = chain_ca_coords[other_chain]['first_residue']['residue']
                    other_chain_last_residue_num = chain_ca_coords[other_chain]['last_residue']['residue']

                    if abs(chain_last_residue_num - other_chain_first_residue_num) < abs(chain_first_residue_num - other_chain_last_residue_num):
                        closest_residue_pair = (chain_last_residue_num, other_chain_first_residue_num)
                        closest_residue_coords_chain = chain_ca_coords[chain]['last_residue']['coords']
                        closest_residue_coords_other_chain = chain_ca_coords[other_chain]['first_residue']['coords']
                    else:
                        closest_residue_pair = (chain_first_residue_num, other_chain_last_residue_num)
                        closest_residue_coords_chain = chain_ca_coords[chain]['first_residue']['coords']
                        closest_residue_coords_other_chain = chain_ca_coords[other_chain]['last_residue']['coords']

                    # Calculate the distance between the closest residues
                    distance = calculate_distance(closest_residue_coords_chain, closest_residue_coords_other_chain)
                    
                    distance_matrix[i, j] = distance

    # Assigning fragmented monomers to the same chain (Only if needed)
    if len(closest_residue_pair) > 0:

        # Apply the Hungarian algorithm
        row_ind, col_ind = linear_sum_assignment(distance_matrix)

        # Initialize list to store final data with closest chains
        closest_chains_data = []

        # Collect the results
        for i, j in zip(row_ind, col_ind):
            if distance_matrix[i, j] < np.inf:
                closest_chains_data.append({
                    'PDB': pdb,
                    'chain': chains[i],
                    'closest_non_overlapping_chain': chains[j],
                    'distance': distance_matrix[i, j],
                    'residue_pair': closest_residue_pair
                })

        # Create a dataframe containing the closest non-overlapping chain for each chain
        closest_chains_df = pd.DataFrame(closest_chains_data)

        # Initialize a set to keep track of chains that have already been mapped
        mapped_chains = set()

        # Initialize a list to store the filtered closest chains data
        filtered_closest_chains_data = []

        # Iterate over the closest_chains_df and filter out redundant mappings
        for _, row in closest_chains_df.iterrows():
            chain = row['chain']
            closest_chain = row['closest_non_overlapping_chain']
            
            # Check if this pair has already been included in the reverse direction
            if closest_chain not in mapped_chains:
                filtered_closest_chains_data.append(row)
                # Mark both chains as mapped
                mapped_chains.add(chain)
                mapped_chains.add(closest_chain)

        # Create a filtered DataFrame
        filtered_closest_chains_df = pd.DataFrame(filtered_closest_chains_data)

        # Create a mapping from each chain to its closest non-overlapping chain
        chain_mapping = {row['closest_non_overlapping_chain']: row['chain'] for _, row in filtered_closest_chains_df.iterrows()}

        # Use the replace method to update the chain names in the original DataFrame
        df['chain'] = df['chain'].replace(chain_mapping)

        ### Saving the fused monomers structure as the published structure PDB ###

        # Update chain information in the loaded structure
        for index, row in df.iterrows():
            selection = f"index {row['atom_number']} and resi {row['residue']}"
            cmd.alter(selection, f"chain = '{row['chain']}'")

        # Save the modified structure as a new PDB file
        cmd.save(os.path.join("Output","PDBs", "published_structure", pdb + ".pdb"), pdb)

    ##############################
    ### CALCULATING COM AND Rg ###
    ##############################

   # Initialize list to store final data
    com_rg_data = []

    # Calculate center of mass and radius of gyration for each chain
    for chain in df['chain'].unique():
        chain_coords = df[df['chain'] == chain]
        com = calculate_center_of_mass(chain_coords)
        rg = calculate_radius_of_gyration(chain_coords, com)
        com_rg_data.append({
            'PDB': pdb,
            'chain': chain,
            'x_com': com[0],
            'y_com': com[1],
            'z_com': com[2],
            'Rg': rg
        })

    # Create a dataframe containing the COM and Rg for each chain
    current_pdb_df = pd.DataFrame(com_rg_data)

    ####################################################
    ### CALCULATING DISTANCE BETWEEN EACH CHAINS COM ###
    ####################################################

    # Calculate distances between each pair of chains' COM
    distance_data = []
    num_chains = len(com_rg_data)
    for i in range(num_chains):
        for j in range(num_chains):
            if i != j:
                chain_i = com_rg_data[i]['chain']
                chain_j = com_rg_data[j]['chain']
                com_i = np.array([com_rg_data[i]['x_com'], com_rg_data[i]['y_com'], com_rg_data[i]['z_com']])
                com_j = np.array([com_rg_data[j]['x_com'], com_rg_data[j]['y_com'], com_rg_data[j]['z_com']])
                distance = np.linalg.norm(com_i - com_j)
                distance_data.append({
                    'PDB': pdb,
                    'chain_1': chain_i,
                    'chain_2': chain_j,
                    'distance': distance
                })

    # Create a DataFrame for distances
    current_distance_df = pd.DataFrame(distance_data)

    ############################################
    ### CALCULATING CHAIN DISTANCE THRESHOLD ###
    ############################################
    # Keep only the minimum distance value for each unique value in chain_1
    if len(current_distance_df) > 0:
        idx = current_distance_df.groupby('chain_1')['distance'].idxmin()
        current_min_distance = current_distance_df.loc[idx].reset_index(drop=True)
            
        print("\n Minimum distances:\n")
        print(current_min_distance)

        mean_distance = current_min_distance['distance'].mean()

        distance_threshold = mean_distance + (mean_distance * 0.5)

    else:
        distance_threshold = 4

    # Single protofilament structures can have a very low mean so setting a minimum value
    if distance_threshold < 4:
        distance_threshold = 4

    # Print the mean distance
    print("\nDistance Threshold = ", distance_threshold)


    ################################################################
    ### IDENTIFYING NUMBER OF FIBRILS AND WHICH CHAIN IS ON EACH ###
    ################################################################

   # Initialize a new column 'fibril' with NaN
    current_pdb_df['fibril'] = np.nan

    # Initialize fibril_number
    fibril_number = 1

    # Loop to assign fibril numbers
    for i in range(len(current_pdb_df)):
        # If the current chain is not yet assigned to a fibril
        if np.isnan(current_pdb_df.loc[i, 'fibril']):
            # Assign the current fibril number to this chain
            current_pdb_df.loc[i, 'fibril'] = fibril_number
            updated = True
            
            # Continue updating until no more chains can be added to the current fibril
            while updated:
                updated = False
                # Loop through all chains to find chains that belong to the current fibril
                for k in range(len(current_pdb_df)):
                    # If the chain k is already assigned to the current fibril
                    if not np.isnan(current_pdb_df.loc[k, 'fibril']) and current_pdb_df.loc[k, 'fibril'] == fibril_number:
                        # Loop through all chains to check for potential additions to the fibril
                        for j in range(len(current_pdb_df)):
                            # If the chain j is not yet assigned to any fibril
                            if np.isnan(current_pdb_df.loc[j, 'fibril']):
                                # Loop through all chains currently in the fibril to check proximity
                                for l in current_pdb_df[current_pdb_df['fibril'] == fibril_number].index:
                                    # If the chain j can be part of the fibril based on the distance threshold
                                    if can_be_in_fibril(l, j, distance_threshold):
                                        # Assign the fibril number to chain j
                                        current_pdb_df.loc[j, 'fibril'] = fibril_number
                                        # Mark as updated to continue the while loop
                                        updated = True
                                        # Break the loop to check the next unassigned chain
                                        break
            
            # Move to the next fibril number for the next unassigned point
            fibril_number += 1

    print("\n", current_pdb_df, "\n")

    ###############################################
    ### IDENTIFYING DISTINCT FOLDS WITHIN PDBS ###
    ###############################################

    # Calculate mean Rg for each fibril
    fibril_mean_Rg = current_pdb_df.groupby('fibril')['Rg'].mean().reset_index()
    fibril_mean_Rg.columns = ['fibril', 'mean_Rg']

    print("Fibril Mean Rg = \n", fibril_mean_Rg)

    # Calculate 5% confidence intervals for each fibril's mean Rg
    fibril_mean_Rg['lower_bound'] = fibril_mean_Rg['mean_Rg'] * 0.95
    fibril_mean_Rg['upper_bound'] = fibril_mean_Rg['mean_Rg'] * 1.05

    # Initialize lists to store selected chains
    selected_chains = []

##################################################################################
#################### Solution to handle distinct fibril pairs ####################
##################################################################################

    # Cluster fibrils by overlapping mean Rg intervals
    n_fibrils = len(fibril_mean_Rg)
    adjacency = np.zeros((n_fibrils, n_fibrils), dtype=bool)

    for i, fibril_i in fibril_mean_Rg.iterrows():
        for j, fibril_j in fibril_mean_Rg.iterrows():
            # Overlap if either mean is within the other's interval
            if (
                (fibril_i['lower_bound'] <= fibril_j['mean_Rg'] <= fibril_i['upper_bound']) or
                (fibril_j['lower_bound'] <= fibril_i['mean_Rg'] <= fibril_j['upper_bound'])
            ):
                adjacency[i, j] = True

    # Find connected components (clusters)
    from collections import deque

    visited = [False] * n_fibrils
    clusters = []
    for i in range(n_fibrils):
        if not visited[i]:
            cluster = [i]
            queue = deque([i])
            visited[i] = True
            while queue:
                idx = queue.popleft()
                for j in range(n_fibrils):
                    if adjacency[idx, j] and not visited[j]:
                        visited[j] = True
                        cluster.append(j)
                        queue.append(j)
            clusters.append(cluster)

    # Assign polymorph numbers
    fibril_mean_Rg['polymorph'] = 0
    for poly_num, cluster in enumerate(clusters, 1):
        for idx in cluster:
            fibril_mean_Rg.loc[idx, 'polymorph'] = poly_num

    # Map polymorph numbers back to current_pdb_df
    current_pdb_df['polymorph'] = current_pdb_df['fibril'].map(
        dict(zip(fibril_mean_Rg['fibril'], fibril_mean_Rg['polymorph']))
    ).astype(int).astype(str)

    # Create the 'pdb_id' column based on the number of polymorphs
    if len(clusters) > 1:
        current_pdb_df['pdb_id'] = current_pdb_df['PDB'] + '_' + current_pdb_df['polymorph']
    else:
        current_pdb_df['pdb_id'] = current_pdb_df['PDB']

    # Reconstruct distinct_fibrils: first fibril from each cluster
    distinct_fibrils = []
    for cluster in clusters:
        # Get the fibril number (not index) for the first member of each cluster
        fibril_number = fibril_mean_Rg.iloc[cluster[0]]['fibril']
        distinct_fibrils.append(fibril_number)

##################################################################################
##################################################################################
##################################################################################

    ### Handle the case where there are distinct fibrils ###

    if distinct_fibrils:
        for fibril in distinct_fibrils:
            selected_chain = select_chain_with_lowest_z_com(fibril)
            selected_chains.append(selected_chain)
    else:
        # If no completely distinct fibrils, handle pairs with similar Rg values
        processed_fibrils = set()
        for fibril, similar_fibrils in similar_pairs:
            if fibril not in processed_fibrils:
                processed_fibrils.add(fibril)
                for similar_fibril in similar_fibrils:
                    processed_fibrils.add(similar_fibril)
                # Select one chain with the lowest z_com from this group of similar fibrils
                similar_group = [fibril] + similar_fibrils
                selected_chain = select_chain_with_lowest_z_com(similar_group[0])  # Choose the first one as the representative
                selected_chains.append(selected_chain)

    # Save the final selected chains, ensuring at least one is saved
    if not selected_chains:
        selected_chain = select_chain_with_lowest_z_com(current_pdb_df['fibril'].unique()[0])
        selected_chains = [selected_chain]

    # Save each selected chain to its own file named as pdb_1.pdb, pdb_2.pdb, etc. if there are more than oen for a singe pdb
    if len(selected_chains) > 1:
        for i, chain in enumerate(selected_chains, 1):
            save_selected_chains([chain], os.path.join("Output", "PDBs", "unique_chains", f"{pdb}_{i}.pdb"))

    else:
        save_selected_chains(selected_chains, os.path.join("Output", "PDBs", "unique_chains", f"{pdb}.pdb"))

    # Display selected chains for verification
    print("\nSelected unique chains:", selected_chains)

    ##############################################
    ### SAVING THE BOTTOM CHAIN OF EACH FIBRIL ###
    ##############################################

    # Save a new .pdb file that contains one chain from each fibril with the lowest z_com
    asym_unit_chains = []
    for fibril in current_pdb_df['fibril'].unique():
        fibril_chains = current_pdb_df[current_pdb_df['fibril'] == fibril]
        selected_chain = fibril_chains.loc[fibril_chains['z_com'].idxmin(), 'chain']
        asym_unit_chains.append(selected_chain)

    save_selected_chains(asym_unit_chains, os.path.join("Output", "PDBs", "asymetric_unit", f"{pdb}_asym_unit.pdb"))

    # Display the chains selected for the asymmetric unit
    print("\nAsymmetric unit chains:", asym_unit_chains)   

    ####################################
    ### Adding current df to main df ###
    ####################################

    # Append current DataFrames to the initialized ones
    com_and_fibril_df = pd.concat([com_and_fibril_df, current_pdb_df], ignore_index=True)
    com_distances_df = pd.concat([com_distances_df, current_distance_df], ignore_index=True)
    min_distance_df = pd.concat([min_distance_df, current_min_distance], ignore_index=True)

    ###################
    ### Cleaning up ###
    ###################

    # Delete everything to clean the session for the next PDB
    cmd.delete('all')

# Clean up PyMOL
for i in pdb:
    pymol.cmd.delete(i)

# Close PyMol
cmd.quit()

# Deleting the .cif files that automatically save
for filename in os.listdir():
    if filename.endswith(".cif"):
        file_path = os.path.join(filename)
        os.remove(file_path)

###################
### SAVING DATA ###
###################
com_and_fibril_df.to_csv(os.path.join("Output", "PDBs", "COM_and_fibril.csv"), index = False)
com_distances_df.to_csv(os.path.join("Output", "PDBs", "COM_distances.csv"), index = False)
min_distance_df.to_csv(os.path.join("Output", "PDBs", "min_COM_distances.csv"), index = False)
